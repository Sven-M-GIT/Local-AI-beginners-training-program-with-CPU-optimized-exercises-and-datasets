# NLP Tokenization

import nltk
nltk.download('punkt')
from nltk.tokenize import word_tokenize

text = "Generative AI is transforming industries."
tokens = word_tokenize(text)
print("Tokens:", tokens)

### Exercises:
- Tokenize a paragraph of your choice.
- Count word frequencies.
- Identify stop words and remove them.
